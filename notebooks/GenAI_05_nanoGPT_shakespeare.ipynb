{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAKESPEARE, CERVANTES\n",
    "\n",
    "This is a small project utilizing the transformer architecture to generate new SHAKESPEARE or CERVANTES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0   113k      0  0:00:09  0:00:09 --:--:-- 99272\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2133k  100 2133k    0     0   652k      0  0:00:03  0:00:03 --:--:--  652k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/ajmaradiaga/cervantes-text-generation/master/dataset/DonQuixote.txt -o DonQuixote.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  986k  100  986k    0     0   782k      0  0:00:01  0:00:01 --:--:--  782k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/ajmaradiaga/cervantes-text-generation/master/dataset/ExemplaryNovels.txt -o ExemplaryNovels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 3000\n",
    "eval_interval = 300\n",
    "lr = 0.01\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ['shakespeare.txt', 'DonQuixote.txt', 'ExemplaryNovels.txt']\n",
    "with open('../data/' + input[0], 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_length = len(vocab)\n",
    "print(vocab_length, ''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi = {c:i for i,c in enumerate(vocab)}\n",
    "itoc = {i:c for i,c in enumerate(vocab)}\n",
    "encode = lambda s: [ctoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itoc[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: 1\n",
      "&: 3\n",
      "3: 27\n",
      "X: 112\n",
      "Z: 198\n",
      "Q: 231\n",
      "J: 320\n",
      "z: 356\n",
      "x: 529\n",
      "q: 609\n",
      "j: 628\n",
      "V: 798\n",
      "K: 1584\n",
      "P: 1641\n",
      "Y: 1718\n",
      "F: 1797\n",
      "-: 1897\n",
      "D: 2089\n",
      "!: 2172\n",
      "G: 2399\n",
      "?: 2462\n",
      "B: 2761\n",
      "M: 2840\n",
      "H: 3068\n",
      "U: 3313\n",
      "W: 3530\n",
      ";: 3628\n",
      "C: 3820\n",
      "L: 3876\n",
      "S: 4523\n",
      "R: 4869\n",
      "N: 5079\n",
      "O: 5481\n",
      "E: 6041\n",
      "': 6187\n",
      "T: 7015\n",
      "k: 7088\n",
      "v: 7793\n",
      "A: 7819\n",
      ".: 7885\n",
      ":: 10316\n",
      "p: 10808\n",
      "b: 11321\n",
      "I: 11832\n",
      "g: 13356\n",
      "c: 15623\n",
      "f: 15770\n",
      "w: 17585\n",
      ",: 19846\n",
      "y: 20448\n",
      "m: 22243\n",
      "u: 26584\n",
      "d: 31358\n",
      "l: 33339\n",
      "\n",
      ": 40000\n",
      "i: 45537\n",
      "n: 48529\n",
      "r: 48889\n",
      "s: 49696\n",
      "h: 51310\n",
      "a: 55507\n",
      "o: 65798\n",
      "t: 67009\n",
      "e: 94611\n",
      " : 169892\n"
     ]
    }
   ],
   "source": [
    "counts = {c:0 for c in vocab}\n",
    "for c in text:\n",
    "    counts[c] += 1\n",
    "\n",
    "print(*[f'{c}: {counts[c]}' for c in sorted(counts, key=lambda x: counts[x])], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([32, 8])\n",
      "tensor([[43, 61,  1, 58, 46, 63,  1, 45],\n",
      "        [39, 58, 43, 56,  6,  1, 61, 46],\n",
      "        [ 1, 58, 46, 43,  1, 54, 53, 47],\n",
      "        [45, 43, 57,  1, 46, 53, 56, 57],\n",
      "        [56, 43,  6,  1, 46, 47, 57,  1],\n",
      "        [ 1, 51, 43, 52,  1, 53, 44,  1],\n",
      "        [14, 59, 58,  6,  1, 47, 44,  1],\n",
      "        [ 6,  1, 58, 46, 39, 58,  1, 42],\n",
      "        [ 1, 52, 53, 52, 43,  8,  0,  0],\n",
      "        [53, 42, 63,  5, 57,  1, 51, 53],\n",
      "        [57, 54, 43, 39, 49,  1, 51, 53],\n",
      "        [41, 53, 50, 42, 10,  0, 21, 44],\n",
      "        [43, 57,  6,  0, 21,  1, 50, 53],\n",
      "        [59, 56, 50, 63,  1, 57, 63, 52],\n",
      "        [60, 43,  1, 58, 53,  1, 58, 46],\n",
      "        [53, 59,  1, 39, 56, 58,  1, 45],\n",
      "        [43, 50,  1, 41, 56, 59, 43, 50],\n",
      "        [41, 53, 50, 53, 59, 56, 57,  2],\n",
      "        [57,  1, 57, 59, 41, 46,  1, 57],\n",
      "        [58, 46, 43, 47, 56,  1, 61, 47],\n",
      "        [63, 53, 59,  0, 13, 57,  1, 44],\n",
      "        [50, 53, 53, 42,  1, 39, 52, 42],\n",
      "        [52, 41, 43,  1, 46, 39, 58, 46],\n",
      "        [ 0, 14, 53, 58, 46, 10,  0, 35],\n",
      "        [ 0, 35, 43,  1, 57, 46, 39, 50],\n",
      "        [43, 42, 43, 50, 47, 60, 43, 56],\n",
      "        [53, 61,  5, 42,  1, 40, 43, 44],\n",
      "        [63, 53, 59,  8,  0,  0, 15, 50],\n",
      "        [43,  1, 46, 39, 42,  1, 53, 52],\n",
      "        [ 0, 26, 53, 52, 43,  6,  1, 57],\n",
      "        [46, 47, 51, 57, 43, 50, 44, 11],\n",
      "        [47, 43, 50, 42, 47, 52, 45,  1]])\n",
      "\n",
      "outputs: torch.Size([32, 8])\n",
      "tensor([[61,  1, 58, 46, 63,  1, 45, 56],\n",
      "        [58, 43, 56,  6,  1, 61, 46, 43],\n",
      "        [58, 46, 43,  1, 54, 53, 47, 57],\n",
      "        [43, 57,  1, 46, 53, 56, 57, 43],\n",
      "        [43,  6,  1, 46, 47, 57,  1, 22],\n",
      "        [51, 43, 52,  1, 53, 44,  1, 61],\n",
      "        [59, 58,  6,  1, 47, 44,  1, 43],\n",
      "        [ 1, 58, 46, 39, 58,  1, 42, 47],\n",
      "        [52, 53, 52, 43,  8,  0,  0, 19],\n",
      "        [42, 63,  5, 57,  1, 51, 53, 47],\n",
      "        [54, 43, 39, 49,  1, 51, 53, 56],\n",
      "        [53, 50, 42, 10,  0, 21, 44,  1],\n",
      "        [57,  6,  0, 21,  1, 50, 53, 60],\n",
      "        [56, 50, 63,  1, 57, 63, 52, 53],\n",
      "        [43,  1, 58, 53,  1, 58, 46, 43],\n",
      "        [59,  1, 39, 56, 58,  1, 45, 53],\n",
      "        [50,  1, 41, 56, 59, 43, 50,  1],\n",
      "        [53, 50, 53, 59, 56, 57,  2,  1],\n",
      "        [ 1, 57, 59, 41, 46,  1, 57, 41],\n",
      "        [46, 43, 47, 56,  1, 61, 47, 58],\n",
      "        [53, 59,  0, 13, 57,  1, 44, 53],\n",
      "        [53, 53, 42,  1, 39, 52, 42,  1],\n",
      "        [41, 43,  1, 46, 39, 58, 46,  1],\n",
      "        [14, 53, 58, 46, 10,  0, 35, 46],\n",
      "        [35, 43,  1, 57, 46, 39, 50, 50],\n",
      "        [42, 43, 50, 47, 60, 43, 56,  1],\n",
      "        [61,  5, 42,  1, 40, 43, 44, 53],\n",
      "        [53, 59,  8,  0,  0, 15, 50, 53],\n",
      "        [ 1, 46, 39, 42,  1, 53, 52, 43],\n",
      "        [26, 53, 52, 43,  6,  1, 57, 47],\n",
      "        [47, 51, 57, 43, 50, 44, 11,  0],\n",
      "        [43, 50, 42, 47, 52, 45,  1, 59]])\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(data.size(0) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f'inputs: {xb.shape}\\n{xb}\\n\\noutputs: {yb.shape}\\n{yb}\\n-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'e' -> 'w'\n",
      "'ew' -> ' '\n",
      "'ew ' -> 't'\n",
      "'ew t' -> 'h'\n",
      "'ew th' -> 'y'\n",
      "'ew thy' -> ' '\n",
      "'ew thy ' -> 'g'\n",
      "'ew thy g' -> 'r'\n",
      "'a' -> 't'\n",
      "'at' -> 'e'\n",
      "'ate' -> 'r'\n",
      "'ater' -> ','\n",
      "'ater,' -> ' '\n",
      "'ater, ' -> 'w'\n",
      "'ater, w' -> 'h'\n",
      "'ater, wh' -> 'e'\n",
      "' ' -> 't'\n",
      "' t' -> 'h'\n",
      "' th' -> 'e'\n",
      "' the' -> ' '\n",
      "' the ' -> 'p'\n",
      "' the p' -> 'o'\n",
      "' the po' -> 'i'\n",
      "' the poi' -> 's'\n",
      "'g' -> 'e'\n",
      "'ge' -> 's'\n",
      "'ges' -> ' '\n",
      "'ges ' -> 'h'\n",
      "'ges h' -> 'o'\n",
      "'ges ho' -> 'r'\n",
      "'ges hor' -> 's'\n",
      "'ges hors' -> 'e'\n",
      "'r' -> 'e'\n",
      "'re' -> ','\n",
      "'re,' -> ' '\n",
      "'re, ' -> 'h'\n",
      "'re, h' -> 'i'\n",
      "'re, hi' -> 's'\n",
      "'re, his' -> ' '\n",
      "'re, his ' -> 'J'\n",
      "' ' -> 'm'\n",
      "' m' -> 'e'\n",
      "' me' -> 'n'\n",
      "' men' -> ' '\n",
      "' men ' -> 'o'\n",
      "' men o' -> 'f'\n",
      "' men of' -> ' '\n",
      "' men of ' -> 'w'\n",
      "'B' -> 'u'\n",
      "'Bu' -> 't'\n",
      "'But' -> ','\n",
      "'But,' -> ' '\n",
      "'But, ' -> 'i'\n",
      "'But, i' -> 'f'\n",
      "'But, if' -> ' '\n",
      "'But, if ' -> 'e'\n",
      "',' -> ' '\n",
      "', ' -> 't'\n",
      "', t' -> 'h'\n",
      "', th' -> 'a'\n",
      "', tha' -> 't'\n",
      "', that' -> ' '\n",
      "', that ' -> 'd'\n",
      "', that d' -> 'i'\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size//4):\n",
    "    for t in range(block_size):\n",
    "        # print(f'{xb[b,:t+1]} -> {yb[b,t]}')\n",
    "        print(f'{repr(decode(list(xb[b,:t+1].numpy())))} -> {repr(itoc[yb[b,t].item()])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        data = train_data if split == 'train' else val_data\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[i] = loss\n",
    "        out[split] = losses.mean().item()\n",
    "    return out\n",
    "\n",
    "# simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_length):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_length, vocab_length)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        logits = self.embedding(x)\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, vocab_length), targets.view(-1))\n",
    "            return logits, loss\n",
    "        return logits, None\n",
    "        # B, T, C = logits.shape\n",
    "        # logits = logits.view(B*T, C)\n",
    "        # targets = targets.view(B*T)\n",
    "        # loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits, _ = self(idx)\n",
    "                logits = logits[:,-1,:]\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5817, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1744)\n",
      "\n",
      "\n",
      "XaOlDyPjzsAAaUVkb!ywJo\n",
      "IgOuF!3gXfyDIcNmLJ!eCJ?w?H,IE.cxbu,;!QXchFOmT''Zq&&?ob&NbqNkTbGwIfV;S&myazC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = BigramLanguageModel(vocab_length)\n",
    "loss = model(xb, yb)[1]\n",
    "print(loss)\n",
    "print(torch.tensor(vocab_length).float().log())\n",
    "\n",
    "print(decode(model.generate(torch.tensor([[ctoi['\\n']]], dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, train loss: 4.68, val loss: 4.70\n",
      "iteration 300, train loss: 2.81, val loss: 2.84\n",
      "iteration 600, train loss: 2.55, val loss: 2.56\n",
      "iteration 900, train loss: 2.50, val loss: 2.52\n",
      "iteration 1200, train loss: 2.47, val loss: 2.51\n",
      "iteration 1500, train loss: 2.47, val loss: 2.50\n",
      "iteration 1800, train loss: 2.47, val loss: 2.50\n",
      "iteration 2100, train loss: 2.47, val loss: 2.48\n",
      "iteration 2400, train loss: 2.46, val loss: 2.49\n",
      "iteration 2700, train loss: 2.46, val loss: 2.48\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for i in range(max_iters):\n",
    "    # sample batch\n",
    "    x, y = get_batch('train')\n",
    "    # forward pass\n",
    "    logits, loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    # print loss\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(f'iteration {i}, train loss: {losses[\"train\"]:.2f}, val loss: {losses[\"val\"]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y th cta yowshe,\n",
      "SSe; sthatu, his s tis w!\n",
      "PUCon yo Whyof carmed 'dend gulop haifu stiom:\n",
      "IG otodpreve way pongretheellestime, ld w'rr:\n",
      "\n",
      "Me.\n",
      "IOnthad m maird m fely wior. nd ap at, te w mmisell ol' llooust ble'stothithast auen Windengover blorde pre llshis he.\n",
      "\n",
      "\n",
      "Bu t! hatoun n allas.\n",
      "Hayom we fy eand wacano:\n",
      "Ton Wha pte s lounge tik,\n",
      "osuthan Jut g, n burimy s, d IUEThall wn nay h?\n",
      "S:\n",
      "\n",
      "TI athirst, orogever:\n",
      "ARELem do t whe, t tound nel ue panowade CLIIf I bupre ghalloum air amouengrefXENG foukelie\n"
     ]
    }
   ],
   "source": [
    "# generate text\n",
    "context = torch.tensor([[ctoi['\\n']]], dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, 500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
